*Нужно понять контекст сущности чтобы определить кто это по сути, а далее проверять этот контекст, в случае если видим что-то напоминающее нашу сущность, дабы установить, что речь идёт именно о ней*

- **Что такое "Сущность"?**  
    Любой объект, о котором есть данные: человек ("Иван Иванов"), компания ("Яндекс"), город ("Киров"), событие ("Олимпиада 2024"). Важно: в разных источниках сущность может называться по-разному ("Иван", "Иванов И.И.", "Ваня")

Сравнивать дополнительные атрибуты сущности (email, телефон, геолокацию)

**NER:** - детектор сущностей. Он только **называет типы** (человек/компания/город), но не понимает, _кто именно_ это.

**NER (Named Entity Recognition)** — процесс выделения сущностей в тексте: людей, организаций, мест, дат и т.д. Например, в предложении *«Марк работал в Google»* NER найдёт *«Марк»* (персона) и *«Google»* (организация).

---

**NEL** Опирается на контекст и Базы знаний (Wikipedia)

**NEL (Named Entity Linking)** — После выделения сущностей NEL связывает их с уникальными идентификаторами в базе знаний *(KB)*. Например, слово *«Apple»* может быть ссылкой на компанию Apple Inc. (через Wikipedia/DBpedia) или на фрукт — NEL решает, какой вариант подходит по контексту . 

[TagMe](https://tagme.d4science.org/tagme/)
![[Pasted image 20250709134712.png]]

| Этап | Задача                     | Инструмент                                                                   |
| ---- | -------------------------- | ---------------------------------------------------------------------------- |
| NER  | Найти сущности в тексте    | [bert-base-NER](https://huggingface.co/dslim/bert-base-NER)                  |
| NEL  | Указать, что это за объект | [TagMe](https://tagme.d4science.org/tagme/)  [Wikifier](http://wikifier.org) |

## Подходы к извлечению и связыванию сущностей

### Что такое NER и NEL простыми словами

Представьте, что вы читаете новостную статью и хотите быстро найти всех людей, организации и места, упомянутые в тексте. **NER (Named Entity Recognition)** - это как умный маркер, который подчеркивает важные слова разными цветами: имена людей - желтым, компании - синим, города - зеленым.

**Пример NER:**  
Из текста "Президент Путин встретился с главой Apple Тимом Куком в Москве" система выделит:

- **Владимир Путин** - Человек (PER)
    
- **Apple** - Организация (ORG)
    
- **Тим Кук** - Человек (PER)
    
- **Москва** - Место (LOC)
    

**NEL (Named Entity Linking)** идет дальше - он не просто находит слова, а связывает их с конкретными объектами в базе знаний, как продвинутая система GPS для текста.

**Пример NEL:**  
Слово "Киров" в предложении "Я поеду в Киров на выходных" система должна связать именно с городом Киров, а не с человеком по фамилии Киров.

## Альтернативы NER+NEL

### 1. End-to-End Entity Extraction & Disambiguation[Полный цикл поиска и уточнения сущностей]

**Принцип работы простыми словами:**  
Представьте библиотекаря, который одновременно находит нужные книги и сразу же определяет, о чем именно они. Вместо того чтобы сначала собрать все книги, а потом разбираться с их содержанием, он делает это за один подход[6](https://en.wikipedia.org/wiki/Entity_linking).

**Плюсы с примерами:**

- **Отсутствие накопления ошибок**: Как в игре "испорченный телефон" - чем меньше этапов передачи информации, тем меньше искажений. Если первый этап неправильно определил "Яблоко" как фрукт вместо компании, второй этап уже не сможет это исправить.
    
- **Высокая скорость**: Модель ReFinED работает в 60 раз быстрее традиционных подходов - это как разница между пешеходом и автомобилем.
    
- **Лучшее понимание контекста**: Система видит всю картину целиком, как человек, который читает весь абзац, а не отдельные слова.
    

**Минусы с примерами:**

- **Требует больших данных**: Как обучение хирурга - нужны тысячи размеченных примеров операций. Если у вас есть только 100 примеров вместо 100,000, система не сможет хорошо обучиться.
    
- **Сложность адаптации**: Если система обучена на новостных статьях, она может плохо работать с медицинскими текстами - как переводчик, специализирующийся на художественной литературе, который столкнулся с техническими документами.
    

### 2. Open Information Extraction (Open IE)[Открытый сбор информации]

**Принцип работы простыми словами:**  
Как любопытный ребенок, который задает вопросы "кто что сделал?" ко всему подряд, не зная заранее, что именно искать. Система извлекает тройки типа "субъект-действие-объект" из любого текста.

**Плюсы с примерами:**

- **Не требует предварительных знаний**: Как универсальный детектор, который может найти любые отношения. Если в тексте написано "Данте написал Божественную комедию", система выделит это отношение, даже если никто не учил ее искать информацию о писателях.
    
- **Обнаружение новых отношений**: Может найти связи, о которых никто не думал. Например, "вимпами называют слабовзаимодействующие массивные частицы" - система выделит новый термин без предварительной подготовки.
    

**Минусы с примерами:**

- **Много шума**: Как сетка, которая ловит не только рыбу, но и мусор. Из предложения "Стол стоит в комнате" система может выделить малоинформативную связь "стол - стоит - в комнате".
    
- **Нет связи с реальными объектами**: Система знает, что "Данте - писатель", но не может сказать, что это именно Данте Алигьери, а не какой-то другой Данте.
    

### 3. Vector-based Entity Clustering[Группировка сущностей по векторам]

**Принцип работы простыми словами:**  
Как сортировка вещей в шкафу - все похожие предметы складываются в одну кучку. Система превращает упоминания в числовые "отпечатки пальцев" и группирует похожие вместе.

**Плюсы с примерами:**

- **Автоматическое обнаружение групп**: Как магнитные шарики, которые сами притягиваются к похожим. Система может сгруппировать "Путин", "Президент РФ", "Владимир Владимирович" в одну группу без подсказок.
    
- **Визуализация результатов**: Можно увидеть на карте, как группируются упоминания - как схема метро, где видны все станции и связи.
    

**Минусы с примерами:**

- **Проблемы с омонимами**: Как попытка рассортировать фотографии людей только по именам - все "Александры" попадут в одну кучку, даже если это разные люди.
    
- **Нужна ручная разметка**: Как нужен человек, который подпишет каждую кучку вещей - "это летняя одежда", "это зимняя одежда".
    

### 4. Retrieval-Augmented Generation (RAG)[Поиск с генерацией текста]

**Принцип работы простыми словами:**  
Как умный помощник, который сначала ищет информацию в справочнике, а потом дает ответ. Перед тем как сказать что-то о белых медведях, система сначала найдет информацию о них в энциклопедии.

**Плюсы с примерами:**

- **Высокая точность**: Как доктор, который сначала смотрит в медицинский справочник, а потом дает диагноз. Система может ответить: "Сколько стоят билеты на поезд Москва-Сочи на завтра?" найдя актуальную информацию на сайте РЖД.
    
- **Гибкость обновления**: Как библиотека, в которую можно добавлять новые книги без перестройки здания. Можно добавить новые документы без переобучения всей системы.
    
- **Источники информации**: Как научная работа с ссылками - система может показать, откуда взяла информацию.
    

**Минусы с примерами:**

- **Галлюцинации**: Как человек, который выдумывает факты, если не нашел ответа в справочнике. Система может сказать что-то вроде: "Наполеон изобрел интернет", если не найдет правильной информации.
    
- **Сложность настройки**: Как настройка сложного радиоприемника - нужно правильно настроить все компоненты, чтобы получить качественный сигнал.
    

### 5. Rule-plus-Pattern Hybrid[Смешанный подход: правила и шаблоны]

**Принцип работы простыми словами:**  
Как опытный детектив, который использует набор проверенных правил для поиска улик. Если видит паттерн "Доктор ИМЯ", то понимает, что это врач.

**Плюсы с примерами:**

- **Высокая объяснимость**: Как математическая задача с пошаговым решением. Система может объяснить: "Я определил Apple как компанию, потому что перед ней стоит слово 'корпорация'".
    
- **Быстрая адаптация**: Как конструктор Lego - можно быстро добавить новые правила. Если нужно найти новый тип сущности "криптовалюта", достаточно добавить правило: "если слово заканчивается на 'коин' = криптовалюта".
    
- **Низкие затраты**: Как простой калькулятор против суперкомпьютера - работает быстро и не требует мощного оборудования.
    

**Минусы с примерами:**

- **Плохая масштабируемость**: Как попытка запомнить все правила дорожного движения для каждой страны мира. Чем больше правил, тем сложнее ими управлять.
    
- **Много ручной работы**: Как составление словаря вручную - нужно прописать правило для каждого случая. Для поиска всех российских городов нужно будет написать тысячи правил.
    

### 6. Semantic Role Labeling (SRL)[Анализ ролей в предложении]

**Принцип работы простыми словами:**  
Как анализ предложения в школе - система определяет "кто", "что делает", "с кем", "где", "когда". Для предложения "Мария продала книгу Джону" определяет: Мария - агент, продала - действие, книгу - объект, Джону - получатель.

**Плюсы с примерами:**

- **Богатая семантика**: Как подробный анализ фильма вместо простого "понравился/не понравился". Система может определить, что в предложении "Иван разбил окно молотком" - Иван это агент, окно - пациент, молоток - инструмент.
    
- **Полезно для вопросно-ответных систем**: Как GPS, который понимает не только "где находится кафе", но и "как туда добраться на автобусе в дождливую погоду".
    

**Минусы с примерами:**

- **Высокая вычислительная сложность**: Как подробный анализ каждого кадра фильма вместо просмотра трейлера. Система тратит много времени на анализ каждого слова.
    
- **Все еще нужна дизамбигуация**: Как переводчик, который понимает грамматику, но не знает, о каком именно Иване идет речь.
    

### 7. Active Learning[Активное обучение]

**Принцип работы простыми словами:**  
Как прилежный ученик, который задает вопросы именно о том, что не понимает. Система выбирает самые сложные примеры и просит эксперта их объяснить.

**Плюсы с примерами:**

- **Высокая точность**: Как персональный репетитор, который работает именно с вашими слабыми местами. Если система не уверена, что "Яблоко" в контексте "новые технологии" - это компания, она спросит у эксперта.
    
- **Минимум размеченных данных**: Как эффективная диета - результат достигается меньшими усилиями. Вместо разметки 10,000 примеров достаточно разметить 1,000 самых информативных.
    

**Минусы с примерами:**

- **Зависимость от экспертов**: Как зависимость от консультанта - без него система не может развиваться. Если эксперт в отпуске, обучение останавливается.
    
- **Необходимость интерфейса**: Как необходимость построить мост между островами. Нужно создать удобную систему для работы экспертов, что требует дополнительных затрат.
    

### Примеры задач кластеризации

**Простые примеры кластеризации:**

- **Клиенты интернет-магазина**: Группировка покупателей по сумме трат и частоте покупок. Получаем группы: "экономные", "средние", "VIP-клиенты".
    
- **Музыкальные предпочтения**: Spotify группирует пользователей по любимым жанрам и артистам для персональных рекомендаций.
    
- **Геолокация**: Группировка GPS-точек для определения популярных мест посадки в такси у аэропорта.
    
- **Цветовые схемы**: Группировка цветов на фотографии для создания гармоничной палитры интерфейса.
    

### Применение в реальной жизни

**Примеры ИИ в повседневности:**

- **Голосовые помощники**: Siri, Алиса, Google Assistant понимают речь и выполняют команды.
    
- **Рекомендательные системы**: Netflix предлагает фильмы, Spotify - музыку, Amazon - товары на основе ваших предпочтений.
    
- **Камера смартфона**: Автоматическая фокусировка, распознавание лиц, улучшение качества фотографий.
    
- **Навигация**: Google Maps строит оптимальные маршруты, учитывая пробки и дорожную обстановку.
    
- **Банковские системы**: Скоринг кредитоспособности, обнаружение мошеннических операций.
    

### Почему NER + NEL оптимален для озер данных:

**1. Гетерогенность данных:**  
Озера данных содержат разнообразные типы документов и форматы. 

Модульный подход позволяет адаптировать NER-компонент для каждого типа данных независимо

**Простое объяснение:**  
Каждый компонент можно заменить и улучшить независимо. Если появилась лучшая модель для распознавания имен, можно заменить только эту часть.

---

**2. Масштабируемость:**  
Озера данных содержат огромные объемы информации, требующие высокопроизводительных решений. Пайплайн NER → NEL позволяет эффективно обрабатывать большие объемы данных благодаря возможности параллелизации обработки, что ускоряет работу системы.

**Простое объяснение:**  
Озеро данных — это как гигантский склад с миллионом коробок. Если обрабатывать каждую коробку по очереди, это займет годы. NER + NEL работают как команда работников, которые могут одновременно проверять разные коробки. Это делает обработку данных быстрее и позволяет справляться с огромными объемами информации.

---

**3. Эволюция данных:**  
Данные в озерах постоянно обновляются, добавляются новые типы документов или меняются их форматы. Модульная архитектура NER + NEL позволяет легко адаптировать систему к новым данным, не перестраивая её полностью.

**Простое объяснение:**  
Представьте, что на склад с данными постоянно привозят новые коробки, и они все разные. NER + NEL — это как конструктор, где можно быстро поменять или добавить одну деталь, чтобы работать с новыми коробками, не ломая всю систему.

---

**4. Ограниченные ресурсы разметки:**  
В компаниях часто нет больших размеченных датасетов для обучения моделей. Пайплайн NER → NEL эффективен, так как решает узкие задачи и может использовать предобученные модели с минимальной разметкой.

**Простое объяснение:**  
Разметка данных — это как подписывать каждую страницу в книге, чтобы компьютер понял, что там. Это долго и дорого. NER + NEL уже знают, как искать имена или компании, и им нужно лишь немного примеров, чтобы подстроиться под ваши данные.

---

**5. Интеграция с существующими системами:**  
Озера данных часто работают с разными аналитическими системами. Модульный подход NER + NEL упрощает интеграцию, обеспечивая совместимость с другими инструментами.

**Простое объяснение:**  
Озеро данных — это как большой офис, где разные программы должны работать вместе. NER + NEL — это как универсальный адаптер, который легко подключается к другим системам, не создавая проблем.

---

**6. Контроль качества:**  
В корпоративной среде важен контроль качества данных. Пайплайн NER → NEL позволяет отдельно проверять и улучшать каждый компонент, обеспечивая высокую точность.

**Простое объяснение:**  
Если в данных ошибка, это как бракованный товар на складе. NER + NEL позволяют проверять каждую часть работы отдельно, чтобы быстро находить и исправлять ошибки, не трогая всё остальное.

---

**Заключение:**  
Несмотря на недостатки, связанные с накоплением ошибок, классический пайплайн NER → NEL остается наиболее практичным решением для озер данных благодаря своей модульности, масштабируемости и экономической эффективности. Эти преимущества перевешивают недостатки в контексте реальных корпоративных применений, где важны гибкость, прозрачность и возможность постепенной адаптации системы.
## Альтернативы:

NER:

| Метод                             | Описание                                                       | Плюсы                                   | Минусы                                                                                                                   |
| --------------------------------- | -------------------------------------------------------------- | --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **Регулярные выражения**          | Поиск шаблонов в тексте (например, даты, телефоны)             | Простота, быстро, не требует обучения   | Не подходит для сложных сущностей (например: "Министерство науки и высшего образования РФ"), не умеет различать контекст |
| **Словари / газеттиры**           | Поиск в заранее заданных списках слов                          | Простота, контроль                      | Не находит новые имена, проблемы с омонимами                                                                             |
| **Правила (экспертные)**          | Ручные лингвистические правила: шаблоны + грамматика           | Контроль, Интерпретируемость            | Сложно масштабировать, не обрабатывает вариации языка                                                                    |
| **CRF (Условные случайные поля)** | Модель с ручными признаками и контекстной зависимостью         | Гибкая, большая точность                | Требует разметки, Сложнее реализовать                                                                                    |
| **biLSTM-CRF**                    | Двунаправленные нейросети + CRF для NER                        | Большая точность                        | Долгое обучение, Требует GPU и разметки                                                                                  |
| **Transformers (BERT, RoBERTa)**  | Современные модели, учитывающие глобальный контекст            | Мультиязычность                         | Требует ресурсы, Трудности интерпретации                                                                                 |
| **LLM (GPT‑4, LLaMA)**            | Модель "по запросу": "Найди всех людей и организации в тексте" | Без обучения, гибкость, мультиязычность | **Галлюцинации**                                                                                                         |
| **Гибридные методы**              | Комбинация: шаблоны + словари + DL                             | Универсальность, большая точность       | Сложность архитектуры, cложнее поддерживать                                                                              |
NEL:

| Метод                                     | Описание                                                               | Плюсы                                                | Минусы                                                            |
| ----------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------- | ----------------------------------------------------------------- |
| **Простое соответствие (lookup)**         | Поиск в базе по совпадению имени (label/alias)                         | Быстро, просто, Применимо для больших баз (Wikidata) | Не работает при опечатках, много кандидатов с одинаковыми именами |
| **Словарь с ручной аннотацией**           | Каждое упоминание связано вручную                                      | Высокая точность, Контроль                           | Невозможно масштабировать, работа ручками                         |
| **ML‑ранжировщик (BERT + классификатор)** | Обучаем классификатор, который выбирает лучшего кандидата по контексту | Высокая точность, Учитывает контекст + meta          | Требует обучающей выборки, дорого                                 |
| **Пороговая фильтрация + популярность**   | Отбираем топ‑к кандидатов, учитывая частоту ссылок                     | Быстро, просто                                       | Ошибки при редких именах                                          |
| **LLM**                                   | Генеративная модель сама решает, к чему относится сущность             | Быстро, мультиязычность                              | **Галлюцинации**                                                  |
| **Графовые подходы (Knowledge graph)**    | Используем граф знаний: узлы – сущности, связи – отношения             | Рассуждения, контекст                                | Сложно, требует чистую онтологию (аккуратная база знаний)         |
**Варианты замены на LLM:**  
- Задача требует генерации текста или работы с неструктурированными данными  
- Сложная задача, где KB недостаточно (перевод текста с редкими именами)  

**Алгоритмы NEL**:  
  - **AIDA**: использует статистику из Wikipedia для связывания сущностей
  - **BLINK**: использует BERT для контекстного представления сущностей (более современный подход)

**Инструменты**:  
  - **spaCy** (Python): для NER и простого NEL  
  - **Stanford NLP**: мультиязычность  
  - **OpenTapioca**: связывание с Wikidata  

## Примерный план:

![[Pasted image 20250709131834.png]]

 1. NER для извлечения сущностей  
  Используем готовое (например, spaCy, Natasha для русского языка) или обучите свои на специфических терминах (например, названиях патентов или научных терминах)

2. Связывание через KB (Базу знаний)    
  - **Wikipedia/DBpedia**: для общих сущностей
  - **Wikidata**: содержит уникальные ID и связи между сущностями
  - **YAGO**: связывает сущности с фактами из Википедии и WordNet
  - **Специализированные KB**. 

3. Контекст  
  - **Контекст предложения**: **"Java"** в контексте *"язык программирования"* => ссылка на Java (язык), а не на остров  
  - **Статистика из KB**: популярные сущности в определённой области (например, в патентах чаще встречаются компании, а не природные объекты)

4. Формирование истории объекта
  Для отслеживания эволюции сущности во времени:  
  - **Временные метки**: извлечение даты (NER).
  - **Граф связей**: построение графика с сущностями и связями между ними во времени
  - **Пример**: связь между компанией и её технологией из патента 2010 года, а затем упоминание этой технологии в научной статье 2020 года

## Эксперименты:

**Подготовка тестовой базы**

Можно взять **свою мини-базу знаний** (CSV или словарь), например:

```python
# Простая база: две сущности
my_kb = [
    {"id": "Q1", "name": "Владимир Путин", "description": "Президент России"},
    {"id": "Q2", "name": "Москва", "description": "Столица России"}
]
```

```python
import spacy

nlp = spacy.load("en_core_web_sm")

entity_kb = {
    "Vladimir Putin": {
        "description": "Russian politician, President of Russia.",
        "wikipedia": "https://en.wikipedia.org/wiki/Vladimir_Putin"
    }
}

def perform_ner(text):
    doc = nlp(text)
    print("=== NER ===")
    for ent in doc.ents:
        print(f"Entity: {ent.text}, Label: {ent.label_}")
    return [(ent.text, ent.label_) for ent in doc.ents]

def perform_nel(entities):
    print("=== NEL (Entity Linking) ===")
    for name, label in entities:
        if name in entity_kb:
            info = entity_kb[name]
            print(f"Entity: {name}")
            print(f"  Description: {info['description']}")
            print(f"  Wikipedia: {info['wikipedia']}")
        else:
            print(f"Entity: {name} - no link found")

# Тесты
text1 = "Hi John and Vladimir"
text2 = "Hi John and Vladimir Putin"

ner_entities_1 = perform_ner(text1)
perform_nel(ner_entities_1)

print("\n")

ner_entities_2 = perform_ner(text2)
perform_nel(ner_entities_2)

```


``` python
import spacy  
from kb import entity_kb  
  
nlp = spacy.load("en_core_web_sm")  
  
# Автоиндекс  
name_to_id = {v["name"]: k for k, v in entity_kb.items()}  
  
def perform_ner(text):  
    doc = nlp(text)  
    print("=== NER ===")  
    entities = [(ent.text, ent.label_) for ent in doc.ents]  
    for ent_text, label in entities:  
        print(f"Entity: {ent_text}, Label: {label}")  
    return entities  
  
def resolve_entity_id(name):  
    return name_to_id.get(name)  
  
def show_entity_info_by_id(ent_id, visited=None, level=0):  
    if visited is None:  
        visited = set()  
    if ent_id in visited:  
        return  
    visited.add(ent_id)  
  
    entity = entity_kb[ent_id]  
    indent = "  " * level  
    print(f"{indent}Entity: {entity['name']}")  
    print(f"{indent}  Description: {entity['description']}")  
    print(f"{indent}  Wikipedia: {entity['wikipedia']}")  
  
    for linked_id in entity.get("linked", []):  
        if linked_id in entity_kb:  
            show_entity_info_by_id(linked_id, visited, level + 1)  
  
def perform_nel(entities):  
    print("=== NEL (Entity Linking) ===")  
    for name, label in entities:  
        ent_id = resolve_entity_id(name)  
        if ent_id:  
            show_entity_info_by_id(ent_id)  
        else:  
            print(f"Entity: {name} - no link found")  
  
  
text = "Elon Musk and Vladimir Putin met in the USA"  
  
print("="*50)  
print(text)  
print("="*50)  
entities = perform_ner(text)  
perform_nel(entities)
```


## Проблемы модели:

### Ошибки NER

#### 1. `en_core_web_sm` — **маленькая модель**

- Она не содержит word vectors (векторов слов);
    
- Использует только правила и небольшое количество статистики;
    
- Соответственно, **часто ошибается на реальных текстах**.
    

#### 2. `n't` (часть от **"didn't"**) ошибочно выделяется как сущность

Модель может **ошибочно воспринять `n't` как аббревиатуру страны** или что-то подобное.


### Контекст:

Модель `spaCy` в нашем коде _пытается_ учитывать контекст, но **не очень глубоко**, особенно если используется `en_core_web_sm`.

Модель смотрит на соседние слова. Например:

```
1. "Apple released a new iPhone" → ORG (компания)
2. "I ate an apple for lunch"    → FRUIT (не будет определено как сущность вообще)
```
Модель видит глагол "released" + "iPhone" и делает вывод, что это **компания**.  
Во втором случае "ate" и "lunch" → значит **фрукт**.

#### Контекст учитывается **на поверхности**.

Маленькие модели (`en_core_web_sm`) работают не на глубоком контексте, а на:

- POS-тегах
    
- шаблонах
    
- поверхностной статистике
    

Это значит, что в более сложных случаях они ошибаются:

```
"Steve Jobs founded Apple." → ОК  
"He threw the apple at the wall." → ОК  
"But: 'Apple was delicious today.' → может ошибиться и выдать ORG

```

https://apnews.com/article/musk-putin-x-trump-tesla-election-russia-9cecb7cb0f23ccce49336771280ae179?utm_source=chatgpt.com

